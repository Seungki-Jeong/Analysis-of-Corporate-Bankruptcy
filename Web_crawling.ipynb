{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import urljoin\n",
    "from konlpy.tag import Okt, Hannanum\n",
    "from tqdm.notebook import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('chromedriver')\n",
    "driver.implicitly_wait(3)\n",
    "driver.get('https://search.naver.com/search.naver?where=news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(name_list):\n",
    "    driver.find_element_by_name('query').send_keys(name_list)\n",
    "    driver.find_element_by_class_name('bt_search').click()\n",
    "    \n",
    "    title = []\n",
    "    news_date = []\n",
    "    word_data = []\n",
    "    \n",
    "    date_re = re.compile(r'\\d\\d\\d\\d.\\d\\d.\\d\\d.')\n",
    "    \n",
    "    while True:\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        title_list = soup.find_all(class_='_sp_each_title')\n",
    "        date = soup.find_all(class_='txt_inline')\n",
    "        word = soup.find_all(class_='type01')[0].find_all('dl')\n",
    "\n",
    "        for i in title_list:\n",
    "            title.append(i.get_text())\n",
    "\n",
    "        for dates in date:\n",
    "            try:\n",
    "                match = date_re.search(dates.get_text())\n",
    "                news_date.append(match.group())\n",
    "            except:\n",
    "                date_re2 = re.compile(r'\\d')\n",
    "                match = date_re2.search(dates.get_text())\n",
    "                m = int(match.group())\n",
    "                year = str(datetime.datetime.today().year)\n",
    "                month = str(datetime.datetime.today().month)\n",
    "                day = str(datetime.datetime.today().day-m)\n",
    "                result = year + '.' + month + '.' + day + '.'\n",
    "                news_date.append(result)\n",
    "\n",
    "        for words in word:\n",
    "            word_data.append(words.find_all('dd')[1].get_text())\n",
    "        try:\n",
    "            if soup.find_all(class_='next')[0].get_text() != '다음페이지':\n",
    "                break\n",
    "            else: \n",
    "                driver.find_element_by_class_name('next').click()\n",
    "                driver.implicitly_wait(1)\n",
    "        except:\n",
    "            if soup.find(class_='next') != '다음페이지':\n",
    "                break\n",
    "    \n",
    "    df = pd.DataFrame(list(zip(title, news_date, word_data)), columns = ['title', 'date','contents'])\n",
    "    df['contents'] = df['contents'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣]\",\" \")\n",
    "    \n",
    "    stopwords = ['을', '로', '의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다''의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "    hannanum = Hannanum()\n",
    "    \n",
    "    contents = []\n",
    "\n",
    "    for sentence in tqdm(df[2]):\n",
    "        temp_x = []\n",
    "        temp_x = hannanum.nouns(sentence)\n",
    "        temp_x = [word for word in temp_x if not word in stopwords]\n",
    "        contents.append(temp_x)\n",
    "        \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(contents)\n",
    "    sequences = tokenizer.texts_to_sequences(contents)\n",
    "    \n",
    "    word_to_index = tokenizer.word_index\n",
    "    index_to_word = {}\n",
    "    for key, value in word_to_index.items():\n",
    "        index_to_word[value] = key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = '에이앤티앤'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.find_element_by_name('query').send_keys(name_list)\n",
    "# driver.find_element_by_class_name('bt_search').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title = []\n",
    "# news_date = []\n",
    "# word_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_re = re.compile(r'\\d\\d\\d\\d.\\d\\d.\\d\\d.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "while True:\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    title_list = soup.find_all(class_='_sp_each_title')\n",
    "    date = soup.find_all(class_='txt_inline')\n",
    "    word = soup.find_all(class_='type01')[0].find_all('dl')\n",
    "\n",
    "    for i in title_list:\n",
    "        title.append(i.get_text())\n",
    "\n",
    "    for dates in date:\n",
    "        try:\n",
    "            match = date_re.search(dates.get_text())\n",
    "            news_date.append(match.group())\n",
    "        except:\n",
    "            date_re2 = re.compile(r'\\d')\n",
    "            match = date_re2.search(dates.get_text())\n",
    "            m = int(match.group())\n",
    "            year = str(datetime.datetime.today().year)\n",
    "            month = str(datetime.datetime.today().month)\n",
    "            day = str(datetime.datetime.today().day-m)\n",
    "            result = year + '.' + month + '.' + day + '.'\n",
    "            news_date.append(result)\n",
    "    \n",
    "    for words in word:\n",
    "        word_data.append(words.find_all('dd')[1].get_text())\n",
    "    try:\n",
    "        if soup.find_all(class_='next')[0].get_text() != '다음페이지':\n",
    "            break\n",
    "        else: \n",
    "            driver.find_element_by_class_name('next').click()\n",
    "            driver.implicitly_wait(1)\n",
    "    except:\n",
    "        if soup.find(class_='next') != '다음페이지':\n",
    "            break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "813 813 813\n"
     ]
    }
   ],
   "source": [
    "# print(len(title), len(news_date), len(word_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(list(zip(title, news_date, word_data)), columns = ['title', 'date','contents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[2] = df[2].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣]\",\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      에이앤티앤          반기보고서 감사의견  의견거절  에이앤티앤은 올해 반기보...\n",
       "1      에이앤티앤        은 상호를 아래스 주식회사로 변경한다고   일 공시했다  합...\n",
       "2      에이앤티앤은 주식회사 티이알 및 특별관계자의 지분율이   에서       로 변동했...\n",
       "3      상장사 에이앤티앤에 대해서는 검찰 고발과 임원 면직 권고  과징금 부과 부과액은 금...\n",
       "4      에이앤티앤        은 회계처리기준 위반 행위로 인해 지난   일 증권선물위원회...\n",
       "                             ...                        \n",
       "807      종목명   에이앤티앤  에이앤티앤 거래정지일               에이앤티앤...\n",
       "808    라이트론  에이앤티앤  썬텍  테라셈 등은 반기검토 감사 의견 비적정  자본잠식률 ...\n",
       "809      종목명   에이앤티앤  에이앤티앤 거래정지일               에이앤티앤...\n",
       "810      종목명   에이앤티앤  에이앤티앤 거래정지일               에이앤티앤...\n",
       "811    경남제약  셀바스    에이앤티앤  인터불스     이나그레이트 등   개사는 신규...\n",
       "Name: 2, Length: 812, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords = ['을', '로', '의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다''의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# hannanum = Hannanum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a3afc512d74a278afe9722b79942a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=812.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# contents = []\n",
    "\n",
    "# for sentence in tqdm(df[2]):\n",
    "#     temp_x = []\n",
    "#     temp_x = hannanum.nouns(sentence)\n",
    "#     temp_x = [word for word in temp_x if not word in stopwords]\n",
    "#     contents.append(temp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(contents)\n",
    "# sequences = tokenizer.texts_to_sequences(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n"
     ]
    }
   ],
   "source": [
    "# print(len(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_to_index = tokenizer.word_index\n",
    "# index_to_word = {}\n",
    "# for key, value in word_to_index.items():\n",
    "#     index_to_word[value] = key\n",
    "# len(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에이앤티앤\n",
      "일\n",
      "코오롱티슈진\n",
      "거래정지\n",
      "종목명\n",
      "지정\n",
      "사유\n",
      "공시\n",
      "등\n"
     ]
    }
   ],
   "source": [
    "# for i in range(1,10):\n",
    "#     print(index_to_word[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
